{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Cleaning\n",
    "This section was completed using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Data\n",
    "We will now clean the Sentiment Analysis Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import shutil\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "df = pd.read_csv('../../../data/00-raw-data/Tweets.csv')\n",
    "df_subset = df[['text','sentiment']]\n",
    "df_subset = df_subset.dropna()\n",
    "df_subset = df_subset.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 2)\n",
      "Index(['text', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_subset.shape)\n",
    "print(df_subset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will go ahead and clean our tweets up by removing unneccesary characters and changes in capitalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " I`d have responded, if I were going \n",
      "\n",
      "id have responded if i were going\n",
      "1\n",
      " Sooo SAD I will miss you here in San Diego!!! \n",
      "\n",
      "sooo sad i will miss you here in san diego\n",
      "2\n",
      "my boss is bullying me... \n",
      "\n",
      "my boss is bullying me\n"
     ]
    }
   ],
   "source": [
    "tweets=[]\n",
    "y=[]\n",
    "#ITERATE OVER ROWS\n",
    "# for i in range(0,10):  \n",
    "for i in range(0,len(df_subset)):\n",
    "    # QUICKLY CLEAN TEXT\n",
    "    keep=\"abcdefghijklmnopqrstuvwxyz \"\n",
    "    replace=\".,!;\"\n",
    "    tmp=\"\"\n",
    "    for char in df_subset[\"text\"][i].replace(\"<br />\",\"\").lower():\n",
    "        if char in replace:\n",
    "            tmp+=\" \"\n",
    "        if char in keep:\n",
    "            tmp+=char\n",
    "    tmp=\" \".join(tmp.split())\n",
    "    tweets.append(tmp)\n",
    "    # CONVERT STRINGS TO INT TAGS\n",
    "    if(df_subset[\"sentiment\"][i]==\"positive\"):\n",
    "        y.append(1)\n",
    "    if(df_subset[\"sentiment\"][i]==\"negative\"):\n",
    "        y.append(0)\n",
    "    if(df_subset[\"sentiment\"][i]==\"neutral\"):\n",
    "        y.append(2)\n",
    "    \n",
    "\n",
    "    #PRINT FIRST COUPLE TWEETS\n",
    "    if(i<3):\n",
    "        print(i)\n",
    "        print(df_subset[\"text\"][i].replace(\"<br />\",\"\"),'\\n')\n",
    "        print(tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27480 27480\n"
     ]
    }
   ],
   "source": [
    "#DOUBLE CHECK SIZE\n",
    "y=np.array(y)\n",
    "print(len(tweets),len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will get a CountVectorizer up and going so we further format our tweets for text classification later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to do 1000 features, nothing too much more, otherwise my kernel crashes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer(max_features=1000,stop_words=\"english\")    \n",
    "Xs  =  vectorizer.fit_transform(tweets)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's format our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n",
    "X=np.array(Xs.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs=np.max(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ceil(X/maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab0 = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 1000) (27480,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap keys and values (value --> ley)\n",
    "vocab1 = dict([(value, key) for key, value in vocab0.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420, 329, 772, 706, 552, 432, 474, 81, 409, 60]\n",
      "['id', 'going', 'sooo', 'sad', 'miss', 'interview', 'leave', 'bought', 'httpwww', 'best']\n"
     ]
    }
   ],
   "source": [
    "# CHECK VOCAB KEY-VALUE PAIRS\n",
    "print(list(vocab1.keys())[0:10])\n",
    "print(list(vocab1.values())[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   424  449  189  332  482  210  856  509  961  329  ...  405  555  532  427  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   657  187  749  87   166  893  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n",
    "df2=pd.DataFrame(x)\n",
    "s = df2.sum(axis=0)\n",
    "df2=df2[s.sort_values(ascending=False).index[:]]\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  990  991  992  993  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   994  995  996  997  998  999  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 1000 columns]\n",
      "0      2909.0\n",
      "1      2214.0\n",
      "2      1964.0\n",
      "3      1505.0\n",
      "4      1295.0\n",
      "        ...  \n",
      "995      24.0\n",
      "996      24.0\n",
      "997      24.0\n",
      "998      23.0\n",
      "999      23.0\n",
      "Length: 1000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# RENAME COLUMNS 0,1,2,3 .. \n",
    "df2.columns = range(df2.columns.size)\n",
    "print(df2.head())\n",
    "print(df2.sum(axis=0))\n",
    "x=df2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# REMAP DICTIONARY TO CORRESPOND TO NEW COLUMN NUMBERS\n",
    "print()\n",
    "i1=0\n",
    "vocab2={}\n",
    "for i2 in list(df2.columns):\n",
    "    # print(i2)\n",
    "    vocab2[i1]=vocab1[int(i2)]\n",
    "    i1+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 1000) (27480,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will export our cleaned data to our data folder which we can access later in the Naive Bayes text classification section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(file)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Write each row to the CSV file\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m x:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     writer\u001b[39m.\u001b[39mwriterow(row)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# TWEETS\n",
    "csv_file_path = \"../../../data/01-modified-data/tweet.csv\"\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write each row to the CSV file\n",
    "    for row in x:\n",
    "        writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT\n",
    "csv_file_path = \"../../../data/01-modified-data/sentiment.csv\"\n",
    "np.savetxt(csv_file_path, y, delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Twitter API to Pull in HKJC Tweets\n",
    "Now we will access the Twitter API in order to get Twitter data directly relating to HJKC, specfically the horse Golden Sixty. We will be using the tweepy package for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @HongKong_Racing: 𝑻𝒉𝒆 𝑷𝒓𝒊𝒅𝒆 𝒐𝒇 𝑯𝒐𝒏𝒈 𝑲𝒐𝒏𝒈! 🏅6⃣0⃣\n",
      "\n",
      "GOLDEN SIXTY sets out to make history with a third G1 Hong Kong Mile triumph.\n",
      "\n",
      "#HKIR |…\n",
      "RT @HongKong_Racing: 𝑻𝒉𝒆 𝑷𝒓𝒊𝒅𝒆 𝒐𝒇 𝑯𝒐𝒏𝒈 𝑲𝒐𝒏𝒈! 🏅6⃣0⃣\n",
      "\n",
      "GOLDEN SIXTY sets out to make history with a third G1 Hong Kong Mile triumph.\n",
      "\n",
      "#HKIR |…\n",
      "'He's Done His Job': Hong Kong Superstar Golden Sixty, Now An 8-Year-Old, Readies For International Swansong - Horse Racing News | Paulick Report https://t.co/1NxZ35h75Q\n",
      "@tantric_eden You can be my Santa! Also, you are the biggest golden hearted hoe I have had the pleasure to know. I met my first Ho sixty years ago and in that time I have known a few so I know this to be a fact! https://t.co/GdRCEdhmzb\n",
      "RT @HKJC_Racing: He has over 550 Hong Kong wins and is Golden Sixty's regular rider... 🌟\n",
      "\n",
      "Will @Vincenthocy add a first @LONGINES #IJC titl…\n",
      "RT @HKJC_Racing: 🗣️ \"Great training performance by John Size! Going in cold and winning the Hong Kong Mile!\"\n",
      "\n",
      "First-up after a 189-day brea…\n",
      "Golden Sixty ❓️ https://t.co/4kIuFKXST6\n",
      "He has over 550 Hong Kong wins and is Golden Sixty's regular rider... 🌟\n",
      "\n",
      "Will @Vincenthocy add a first @LONGINES #IJC title to his record!? \n",
      "\n",
      "📍 Happy Valley, 6 Dec | #HKIR | #HKracing https://t.co/pC9kR4DsMR\n",
      "RT @HongKong_Racing: A fairytale finale for GOLDEN SIXTY?\n",
      "\n",
      "@gcunning12 looks ahead to Sunday's G1 HK Mile test for #HKRacing's most decorat…\n",
      "@WHR @HollieDoyle1 @christo68914587 @netkeiba @RacingPost @RacenetTweets @SkyRacingAU @Racing @JRA_WorldRacing @BloodHorse @theTDN @gallop_keiba Just wish he would have competed against golden sixty....🤦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @Hartley_026: 3度目の香港マイル制覇を狙うGolden Sixty。K.ルイ調教師は日本馬を警戒する他、「これが恐らく最後の香港国際競走」ともコメント。\n",
      "\n",
      "こちらは香港ジョッキークラブが寄稿している記事なんですが、「日本での引退が濃厚」とあるので余生を日本で…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @SportingLife: \"If their Golden oldie has one more combustible closer in his locker, then this year’s LONGINES Hong Kong Mile could be a…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @A_Evers: Unreal morning!!\n",
      "\n",
      "Got picked up by Uber at 4:15. Tesla with the plate “OldSport”\n",
      "\n",
      "Golden sixty comes out and just poses for me…\n",
      "RT @HongKong_Racing: A fairytale finale for GOLDEN SIXTY?\n",
      "\n",
      "@gcunning12 looks ahead to Sunday's G1 HK Mile test for #HKRacing's most decorat…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward…\n",
      "Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward is offered for they are gone forever.\n",
      "\n",
      "- HIRAL\n",
      "\n",
      "FUKRA PREACHES POSITIVITY\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "香港カップはRomantic Warrior香港馬にしてオーストラリア遠征でコックスプレートを勝ち国内ではGolden Sixtyに次ぐ圧倒的実力を持つ馬去年の香港カップは衝撃的な勝ち方で日本馬も数頭出るがこの馬を負かすのは大変だ\n",
      "香港マイルはなんと言ってもGolden Sixtyのラストラン今回初めて休み明けで香港マイルに出るから不安はあるんだけどアドマイヤマーズを赤子扱いした3年前の衝撃が忘れられない有終の美を飾って欲しい\n",
      "RT @HKJC_Racing: Ready for a return in Sunday’s Hong Kong Mile? 😏\n",
      "\n",
      "Golden Sixty was buzzing this morning as he moved through his fast work……\n",
      "RT @HKJC_Racing: 🗣️ \"Great training performance by John Size! Going in cold and winning the Hong Kong Mile!\"\n",
      "\n",
      "First-up after a 189-day brea…\n",
      "RT @SportingLife: \"If their Golden oldie has one more combustible closer in his locker, then this year’s LONGINES Hong Kong Mile could be a…\n",
      "🐴 Hong Kong Mile Tips 🐴\n",
      "\n",
      "Can Golden Sixty reclaim his #HongKongMile title?\n",
      "\n",
      "Get the latest horse racing odds for this #ShaTin showdown. \n",
      "\n",
      "➡️ https://t.co/2zyo2c3tNp https://t.co/beeTL6L4DH\n",
      "@GOLDEN_SIXTY_60 テラタクさんそれ昨日のからあげよ〜！！！！！\n",
      "でもからあげは美味しい！！！！！！ビールに合う！！！！！！\n",
      "RT @gcunning12: Ageless Golden Boy or vulnerable Golden Oldie? Either way, Sunday’s Hong Kong Mile is another major test for a horse who ha…\n",
      "\"If their Golden oldie has one more combustible closer in his locker, then this year’s LONGINES Hong Kong Mile could be another timeless classic.\"\n",
      "\n",
      "🏇🇭🇰 Our man in Hong Kong @GCunning12 wonders whether Golden Sixty can win a third LONGINES Hong Kong Mile at Sha Tin this weekend:\n",
      "RT @SportingLife: 📝 \"Although fans of Lucky Sweynesse, Golden Sixty and Romantic Warrior won’t need much encouragement to stay loyal, the t…\n",
      "RT @HKJC_Racing: Ready for a return in Sunday’s Hong Kong Mile? 😏\n",
      "\n",
      "Golden Sixty was buzzing this morning as he moved through his fast work……\n",
      "RT @Racing: 2023 Cox Plate winner, Romantic Warrior, won a barrier trial at Sha Tin this morning with J-Mac in the saddle 😍\n",
      "\n",
      "Golden Sixty f…\n",
      "RT @HutchisHonkers: 🥹Imagine the scenes!\n",
      "\n",
      "R S Dye heaps praise on trainer Francis Lui for his handling of Golden Sixty &amp; can't wait for the…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @gcunning12: Ageless Golden Boy or vulnerable Golden Oldie? Either way, Sunday’s Hong Kong Mile is another major test for a horse who ha…\n",
      "RT @HKJC_Racing: Ready for a return in Sunday’s Hong Kong Mile? 😏\n",
      "\n",
      "Golden Sixty was buzzing this morning as he moved through his fast work……\n",
      "RT @HKJC_Racing: Start your #HKIR week right... 👇\n",
      "\n",
      "Here's the champ, Golden Sixty! 💙🤍💛\n",
      "\n",
      "@LONGINES | @Vincenthocy | #HKracing https://t.co/l…\n",
      "RT @gcunning12: Ageless Golden Boy or vulnerable Golden Oldie? Either way, Sunday’s Hong Kong Mile is another major test for a horse who ha…\n",
      "Ageless Golden Boy or vulnerable Golden Oldie? Either way, Sunday’s Hong Kong Mile is another major test for a horse who has been among the world’s elite for four years now. https://t.co/ABzfyA2I1z\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @Racing: Will Golden Sixty win at #HKIR on Sunday? 👇 https://t.co/4z90XphcAJ\n",
      "RT @SportingLife: 📝 \"Although fans of Lucky Sweynesse, Golden Sixty and Romantic Warrior won’t need much encouragement to stay loyal, the t…\n",
      "Will Golden Sixty win at #HKIR on Sunday? 👇 https://t.co/4z90XphcAJ\n",
      "RT @HongKong_Racing: 𝑻𝒉𝒆 𝑷𝒓𝒊𝒅𝒆 𝒐𝒇 𝑯𝒐𝒏𝒈 𝑲𝒐𝒏𝒈! 🏅6⃣0⃣\n",
      "\n",
      "GOLDEN SIXTY sets out to make history with a third G1 Hong Kong Mile triumph.\n",
      "\n",
      "#HKIR |…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @HongKong_Racing: GOLDEN SIXTY this morning… 🚀🚀\n",
      "\n",
      "Bring on Sunday’s G1 Hong Kong Mile!\n",
      "\n",
      "#HKIR | 10 Dec | #HKRacing\n",
      "\n",
      " https://t.co/SABMWW4…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "Do you give star Singapore galloper Lim’s Kosciuszko any chance of beating Champion galloper Golden Sixty in the Hong Kong Mile on Saturday?\n",
      "\n",
      "https://t.co/V7j1O4cNIX\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "@JioCinema Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward is offered for they are gone forever.\n",
      "\n",
      "#AnkitaIsTheBoss #AnkitaLokhande\n",
      "\n",
      "Sg\n",
      "@T_J_Carroll @HKJC_Racing Do the big 3 win? Lucky sweynese,golden sixty and romantic warrior? 11/10,evs and 5/4\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @HongKong_Racing: 𝑻𝒉𝒆 𝑷𝒓𝒊𝒅𝒆 𝒐𝒇 𝑯𝒐𝒏𝒈 𝑲𝒐𝒏𝒈! 🏅6⃣0⃣\n",
      "\n",
      "GOLDEN SIXTY sets out to make history with a third G1 Hong Kong Mile triumph.\n",
      "\n",
      "#HKIR |…\n",
      "RT @HKJC_Racing: 🗣️ \"Great training performance by John Size! Going in cold and winning the Hong Kong Mile!\"\n",
      "\n",
      "First-up after a 189-day brea…\n",
      "🗣️ \"Great training performance by John Size! Going in cold and winning the Hong Kong Mile!\"\n",
      "\n",
      "First-up after a 189-day break, Glorious Days scored a remarkable win at the 2013 @LONGINES #HKIR! 🔥\n",
      "\n",
      "Golden Sixty returns after 224 days on Sunday... 😳 https://t.co/c6XRsuugRs\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @Punters: \"He's done his job.\"\n",
      "\n",
      "Superstar Golden Sixty readies for probable Hong Kong International Races swansong\n",
      "\n",
      "Story 👉 https://t.co…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @A_Evers: Unreal morning!!\n",
      "\n",
      "Got picked up by Uber at 4:15. Tesla with the plate “OldSport”\n",
      "\n",
      "Golden sixty comes out and just poses for me…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @Hartley_026: 3度目の香港マイル制覇を狙うGolden Sixty。K.ルイ調教師は日本馬を警戒する他、「これが恐らく最後の香港国際競走」ともコメント。\n",
      "\n",
      "こちらは香港ジョッキークラブが寄稿している記事なんですが、「日本での引退が濃厚」とあるので余生を日本で…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @Hartley_026: 3度目の香港マイル制覇を狙うGolden Sixty。K.ルイ調教師は日本馬を警戒する他、「これが恐らく最後の香港国際競走」ともコメント。\n",
      "\n",
      "こちらは香港ジョッキークラブが寄稿している記事なんですが、「日本での引退が濃厚」とあるので余生を日本で…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @Hartley_026: 3度目の香港マイル制覇を狙うGolden Sixty。K.ルイ調教師は日本馬を警戒する他、「これが恐らく最後の香港国際競走」ともコメント。\n",
      "\n",
      "こちらは香港ジョッキークラブが寄稿している記事なんですが、「日本での引退が濃厚」とあるので余生を日本で…\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! 😍\n",
      "\n",
      "#ナミュール 🎌 | #競馬 | @LONG…\n",
      "RT @Hartley_026: 3度目の香港マイル制覇を狙うGolden Sixty。K.ルイ調教師は日本馬を警戒する他、「これが恐らく最後の香港国際競走」ともコメント。\n",
      "\n",
      "こちらは香港ジョッキークラブが寄稿している記事なんですが、「日本での引退が濃厚」とあるので余生を日本で…\n",
      "RT @worldsbesthorse: Golden Sixty will try to win his third Longines Hong Kong Mile (G1) on Sunday. Read: https://t.co/uT6OIegN2n https://t…\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAADBsrQEAAAAAgwcAblxSw4ZubJQxyCOK5bbOh5w%3Dz6b9oNnbsJpfpLXZM0avhOqRtDJqHb9DdZi6vFZVWcfJgSCyvs'\n",
    "\n",
    "client = tweepy.Client(bearer_token)\n",
    "\n",
    "# Search Tweets\n",
    "query = \"Golden Sixty\"\n",
    "tweets = client.search_recent_tweets(query=query, max_results=100)\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    print(tweet.text)\n",
    "    if len(tweet.context_annotations) > 0:\n",
    "        print(tweet.context_annotations)\n",
    "\n",
    "df = pd.DataFrame(tweets.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to clean this tweet data up, so that it matches the format of the sentiment data which we will use to train a classifier. This cleaning process largely follows the same process as the cleaning for the sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[['text','id']]\n",
    "tweets_golden=[]\n",
    "y=[]\n",
    "#ITERATE OVER ROWS\n",
    "# for i in range(0,10):  \n",
    "for i in range(0,len(df_subset)):\n",
    "    # QUICKLY CLEAN TEXT\n",
    "    keep=\"abcdefghijklmnopqrstuvwxyz \"\n",
    "    replace=\".,!;\"\n",
    "    tmp=\"\"\n",
    "    for char in df_subset[\"text\"][i].replace(\"<br />\",\"\").lower():\n",
    "        if char in replace:\n",
    "            tmp+=\" \"\n",
    "        if char in keep:\n",
    "            tmp+=char\n",
    "    tmp=\" \".join(tmp.split())\n",
    "    tweets_golden.append(tmp)\n",
    "\n",
    "og_tweets = pd.DataFrame(tweets_golden)\n",
    "\n",
    "csv_file_path = \"../../../data/01-modified-data/goldensixty_uncleaned.csv\"\n",
    "\n",
    "og_tweets.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_golden  =  vectorizer.fit_transform(tweets_golden)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_golden=np.array(Xs_golden.todense())\n",
    "maxs_golden=np.max(X_golden,axis=0)\n",
    "x2 = np.ceil(X_golden/maxs_golden)\n",
    "vocab0_2 = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap keys and values (value --> ley)\n",
    "vocab1_2 = dict([(value, key) for key, value in vocab0_2.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   66   166  124  83   190  116  47   93   239  125  ...  144  123  130  131  \\\n",
      "0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0   \n",
      "4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
      "\n",
      "   132  136  140  142  143  122  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 240 columns]\n"
     ]
    }
   ],
   "source": [
    "#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n",
    "df2_g=pd.DataFrame(x2)\n",
    "s_g = df2_g.sum(axis=0)\n",
    "df2_g=df2_g[s_g.sort_values(ascending=False).index[:]]\n",
    "print(df2_g.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  230  231  232  233  \\\n",
      "0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0   \n",
      "4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
      "\n",
      "   234  235  236  237  238  239  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 240 columns]\n",
      "0      92.0\n",
      "1      82.0\n",
      "2      45.0\n",
      "3      30.0\n",
      "4      29.0\n",
      "       ... \n",
      "235     1.0\n",
      "236     1.0\n",
      "237     1.0\n",
      "238     1.0\n",
      "239     1.0\n",
      "Length: 240, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# RENAME COLUMNS 0,1,2,3 .. \n",
    "df2_g.columns = range(df2_g.columns.size)\n",
    "print(df2_g.head())\n",
    "print(df2_g.sum(axis=0))\n",
    "x_g=df2_g.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can export our data to our data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"../../../data/01-modified-data/goldensixty.csv\"\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write each row to the CSV file\n",
    "    for row in x_g:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
