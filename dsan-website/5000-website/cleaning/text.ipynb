{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Cleaning\n",
    "This section was completed using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Data\n",
    "We will now clean the Sentiment Analysis Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import shutil\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "df = pd.read_csv('../../../data/00-raw-data/Tweets.csv')\n",
    "df_subset = df[['text','sentiment']]\n",
    "df_subset = df_subset.dropna()\n",
    "df_subset = df_subset.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 2)\n",
      "Index(['text', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_subset.shape)\n",
    "print(df_subset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will go ahead and clean our tweets up by removing unneccesary characters and changes in capitalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " I`d have responded, if I were going \n",
      "\n",
      "id have responded if i were going\n",
      "1\n",
      " Sooo SAD I will miss you here in San Diego!!! \n",
      "\n",
      "sooo sad i will miss you here in san diego\n",
      "2\n",
      "my boss is bullying me... \n",
      "\n",
      "my boss is bullying me\n"
     ]
    }
   ],
   "source": [
    "tweets=[]\n",
    "y=[]\n",
    "#ITERATE OVER ROWS\n",
    "# for i in range(0,10):  \n",
    "for i in range(0,len(df_subset)):\n",
    "    # QUICKLY CLEAN TEXT\n",
    "    keep=\"abcdefghijklmnopqrstuvwxyz \"\n",
    "    replace=\".,!;\"\n",
    "    tmp=\"\"\n",
    "    for char in df_subset[\"text\"][i].replace(\"<br />\",\"\").lower():\n",
    "        if char in replace:\n",
    "            tmp+=\" \"\n",
    "        if char in keep:\n",
    "            tmp+=char\n",
    "    tmp=\" \".join(tmp.split())\n",
    "    tweets.append(tmp)\n",
    "    # CONVERT STRINGS TO INT TAGS\n",
    "    if(df_subset[\"sentiment\"][i]==\"positive\"):\n",
    "        y.append(1)\n",
    "    if(df_subset[\"sentiment\"][i]==\"negative\"):\n",
    "        y.append(0)\n",
    "    if(df_subset[\"sentiment\"][i]==\"neutral\"):\n",
    "        y.append(2)\n",
    "    \n",
    "\n",
    "    #PRINT FIRST COUPLE TWEETS\n",
    "    if(i<3):\n",
    "        print(i)\n",
    "        print(df_subset[\"text\"][i].replace(\"<br />\",\"\"),'\\n')\n",
    "        print(tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27480 27480\n"
     ]
    }
   ],
   "source": [
    "#DOUBLE CHECK SIZE\n",
    "y=np.array(y)\n",
    "print(len(tweets),len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will get a CountVectorizer up and going so we further format our tweets for text classification later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to do 1000 features, nothing too much more, otherwise my kernel crashes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer(max_features=1000,stop_words=\"english\")    \n",
    "Xs  =  vectorizer.fit_transform(tweets)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's format our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n",
    "X=np.array(Xs.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs=np.max(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ceil(X/maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab0 = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 1000) (27480,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap keys and values (value --> ley)\n",
    "vocab1 = dict([(value, key) for key, value in vocab0.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420, 329, 772, 706, 552, 432, 474, 81, 409, 60]\n",
      "['id', 'going', 'sooo', 'sad', 'miss', 'interview', 'leave', 'bought', 'httpwww', 'best']\n"
     ]
    }
   ],
   "source": [
    "# CHECK VOCAB KEY-VALUE PAIRS\n",
    "print(list(vocab1.keys())[0:10])\n",
    "print(list(vocab1.values())[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   424  449  189  332  482  210  856  509  961  329  ...  405  555  532  427  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   657  187  749  87   166  893  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n",
    "df2=pd.DataFrame(x)\n",
    "s = df2.sum(axis=0)\n",
    "df2=df2[s.sort_values(ascending=False).index[:]]\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  990  991  992  993  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   994  995  996  997  998  999  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 1000 columns]\n",
      "0      2909.0\n",
      "1      2214.0\n",
      "2      1964.0\n",
      "3      1505.0\n",
      "4      1295.0\n",
      "        ...  \n",
      "995      24.0\n",
      "996      24.0\n",
      "997      24.0\n",
      "998      23.0\n",
      "999      23.0\n",
      "Length: 1000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# RENAME COLUMNS 0,1,2,3 .. \n",
    "df2.columns = range(df2.columns.size)\n",
    "print(df2.head())\n",
    "print(df2.sum(axis=0))\n",
    "x=df2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# REMAP DICTIONARY TO CORRESPOND TO NEW COLUMN NUMBERS\n",
    "print()\n",
    "i1=0\n",
    "vocab2={}\n",
    "for i2 in list(df2.columns):\n",
    "    # print(i2)\n",
    "    vocab2[i1]=vocab1[int(i2)]\n",
    "    i1+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 1000) (27480,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will export our cleaned data to our data folder which we can access later in the Naive Bayes text classification section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(file)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Write each row to the CSV file\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m x:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mcarswell/dsan-5000-project-mattycars/dsan-website/5000-website/cleaning/text.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     writer\u001b[39m.\u001b[39mwriterow(row)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# TWEETS\n",
    "csv_file_path = \"../../../data/01-modified-data/tweet.csv\"\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write each row to the CSV file\n",
    "    for row in x:\n",
    "        writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT\n",
    "csv_file_path = \"../../../data/01-modified-data/sentiment.csv\"\n",
    "np.savetxt(csv_file_path, y, delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Twitter API to Pull in HKJC Tweets\n",
    "Now we will access the Twitter API in order to get Twitter data directly relating to HJKC, specfically the horse Golden Sixty. We will be using the tweepy package for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @HongKong_Racing: ð‘»ð’‰ð’† ð‘·ð’“ð’Šð’…ð’† ð’ð’‡ ð‘¯ð’ð’ð’ˆ ð‘²ð’ð’ð’ˆ! ðŸ…6âƒ£0âƒ£\n",
      "\n",
      "GOLDEN SIXTY sets out to make history with a third G1 Hong Kong Mile triumph.\n",
      "\n",
      "#HKIR |â€¦\n",
      "RT @HongKong_Racing: ð‘»ð’‰ð’† ð‘·ð’“ð’Šð’…ð’† ð’ð’‡ ð‘¯ð’ð’ð’ˆ ð‘²ð’ð’ð’ˆ! ðŸ…6âƒ£0âƒ£\n",
      "\n",
      "GOLDEN SIXTY sets out to make history with a third G1 Hong Kong Mile triumph.\n",
      "\n",
      "#HKIR |â€¦\n",
      "'He's Done His Job': Hong Kong Superstar Golden Sixty, Now An 8-Year-Old, Readies For International Swansong - Horse Racing News | Paulick Report https://t.co/1NxZ35h75Q\n",
      "@tantric_eden You can be my Santa! Also, you are the biggest golden hearted hoe I have had the pleasure to know. I met my first Ho sixty years ago and in that time I have known a few so I know this to be a fact! https://t.co/GdRCEdhmzb\n",
      "RT @HKJC_Racing: He has over 550 Hong Kong wins and is Golden Sixty's regular rider... ðŸŒŸ\n",
      "\n",
      "Will @Vincenthocy add a first @LONGINES #IJC titlâ€¦\n",
      "RT @HKJC_Racing: ðŸ—£ï¸ \"Great training performance by John Size! Going in cold and winning the Hong Kong Mile!\"\n",
      "\n",
      "First-up after a 189-day breaâ€¦\n",
      "Golden Sixty â“ï¸ https://t.co/4kIuFKXST6\n",
      "He has over 550 Hong Kong wins and is Golden Sixty's regular rider... ðŸŒŸ\n",
      "\n",
      "Will @Vincenthocy add a first @LONGINES #IJC title to his record!? \n",
      "\n",
      "ðŸ“ Happy Valley, 6 Dec | #HKIR | #HKracing https://t.co/pC9kR4DsMR\n",
      "RT @HongKong_Racing: A fairytale finale for GOLDEN SIXTY?\n",
      "\n",
      "@gcunning12 looks ahead to Sunday's G1 HK Mile test for #HKRacing's most decoratâ€¦\n",
      "@WHR @HollieDoyle1 @christo68914587 @netkeiba @RacingPost @RacenetTweets @SkyRacingAU @Racing @JRA_WorldRacing @BloodHorse @theTDN @gallop_keiba Just wish he would have competed against golden sixty....ðŸ¤¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @Hartley_026: 3åº¦ç›®ã®é¦™æ¸¯ãƒžã‚¤ãƒ«åˆ¶è¦‡ã‚’ç‹™ã†Golden Sixtyã€‚K.ãƒ«ã‚¤èª¿æ•™å¸«ã¯æ—¥æœ¬é¦¬ã‚’è­¦æˆ’ã™ã‚‹ä»–ã€ã€Œã“ã‚ŒãŒæã‚‰ãæœ€å¾Œã®é¦™æ¸¯å›½éš›ç«¶èµ°ã€ã¨ã‚‚ã‚³ãƒ¡ãƒ³ãƒˆã€‚\n",
      "\n",
      "ã“ã¡ã‚‰ã¯é¦™æ¸¯ã‚¸ãƒ§ãƒƒã‚­ãƒ¼ã‚¯ãƒ©ãƒ–ãŒå¯„ç¨¿ã—ã¦ã„ã‚‹è¨˜äº‹ãªã‚“ã§ã™ãŒã€ã€Œæ—¥æœ¬ã§ã®å¼•é€€ãŒæ¿ƒåŽšã€ã¨ã‚ã‚‹ã®ã§ä½™ç”Ÿã‚’æ—¥æœ¬ã§â€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @SportingLife: \"If their Golden oldie has one more combustible closer in his locker, then this yearâ€™s LONGINES Hong Kong Mile could be aâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @A_Evers: Unreal morning!!\n",
      "\n",
      "Got picked up by Uber at 4:15. Tesla with the plate â€œOldSportâ€\n",
      "\n",
      "Golden sixty comes out and just poses for meâ€¦\n",
      "RT @HongKong_Racing: A fairytale finale for GOLDEN SIXTY?\n",
      "\n",
      "@gcunning12 looks ahead to Sunday's G1 HK Mile test for #HKRacing's most decoratâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "RT @JainMumbaikar: Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No rewardâ€¦\n",
      "Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward is offered for they are gone forever.\n",
      "\n",
      "- HIRAL\n",
      "\n",
      "FUKRA PREACHES POSITIVITY\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "é¦™æ¸¯ã‚«ãƒƒãƒ—ã¯Romantic Warrioré¦™æ¸¯é¦¬ã«ã—ã¦ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢é å¾ã§ã‚³ãƒƒã‚¯ã‚¹ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å‹ã¡å›½å†…ã§ã¯Golden Sixtyã«æ¬¡ãåœ§å€’çš„å®ŸåŠ›ã‚’æŒã¤é¦¬åŽ»å¹´ã®é¦™æ¸¯ã‚«ãƒƒãƒ—ã¯è¡æ’ƒçš„ãªå‹ã¡æ–¹ã§æ—¥æœ¬é¦¬ã‚‚æ•°é ­å‡ºã‚‹ãŒã“ã®é¦¬ã‚’è² ã‹ã™ã®ã¯å¤§å¤‰ã \n",
      "é¦™æ¸¯ãƒžã‚¤ãƒ«ã¯ãªã‚“ã¨è¨€ã£ã¦ã‚‚Golden Sixtyã®ãƒ©ã‚¹ãƒˆãƒ©ãƒ³ä»Šå›žåˆã‚ã¦ä¼‘ã¿æ˜Žã‘ã§é¦™æ¸¯ãƒžã‚¤ãƒ«ã«å‡ºã‚‹ã‹ã‚‰ä¸å®‰ã¯ã‚ã‚‹ã‚“ã ã‘ã©ã‚¢ãƒ‰ãƒžã‚¤ãƒ¤ãƒžãƒ¼ã‚ºã‚’èµ¤å­æ‰±ã„ã—ãŸ3å¹´å‰ã®è¡æ’ƒãŒå¿˜ã‚Œã‚‰ã‚Œãªã„æœ‰çµ‚ã®ç¾Žã‚’é£¾ã£ã¦æ¬²ã—ã„\n",
      "RT @HKJC_Racing: Ready for a return in Sundayâ€™s Hong Kong Mile? ðŸ˜\n",
      "\n",
      "Golden Sixty was buzzing this morning as he moved through his fast workâ€¦â€¦\n",
      "RT @HKJC_Racing: ðŸ—£ï¸ \"Great training performance by John Size! Going in cold and winning the Hong Kong Mile!\"\n",
      "\n",
      "First-up after a 189-day breaâ€¦\n",
      "RT @SportingLife: \"If their Golden oldie has one more combustible closer in his locker, then this yearâ€™s LONGINES Hong Kong Mile could be aâ€¦\n",
      "ðŸ´ Hong Kong Mile Tips ðŸ´\n",
      "\n",
      "Can Golden Sixty reclaim his #HongKongMile title?\n",
      "\n",
      "Get the latest horse racing odds for this #ShaTin showdown. \n",
      "\n",
      "âž¡ï¸ https://t.co/2zyo2c3tNp https://t.co/beeTL6L4DH\n",
      "@GOLDEN_SIXTY_60 ãƒ†ãƒ©ã‚¿ã‚¯ã•ã‚“ãã‚Œæ˜¨æ—¥ã®ã‹ã‚‰ã‚ã’ã‚ˆã€œï¼ï¼ï¼ï¼ï¼\n",
      "ã§ã‚‚ã‹ã‚‰ã‚ã’ã¯ç¾Žå‘³ã—ã„ï¼ï¼ï¼ï¼ï¼ï¼ãƒ“ãƒ¼ãƒ«ã«åˆã†ï¼ï¼ï¼ï¼ï¼ï¼\n",
      "RT @gcunning12: Ageless Golden Boy or vulnerable Golden Oldie? Either way, Sundayâ€™s Hong Kong Mile is another major test for a horse who haâ€¦\n",
      "\"If their Golden oldie has one more combustible closer in his locker, then this yearâ€™s LONGINES Hong Kong Mile could be another timeless classic.\"\n",
      "\n",
      "ðŸ‡ðŸ‡­ðŸ‡° Our man in Hong Kong @GCunning12 wonders whether Golden Sixty can win a third LONGINES Hong Kong Mile at Sha Tin this weekend:\n",
      "RT @SportingLife: ðŸ“ \"Although fans of Lucky Sweynesse, Golden Sixty and Romantic Warrior wonâ€™t need much encouragement to stay loyal, the tâ€¦\n",
      "RT @HKJC_Racing: Ready for a return in Sundayâ€™s Hong Kong Mile? ðŸ˜\n",
      "\n",
      "Golden Sixty was buzzing this morning as he moved through his fast workâ€¦â€¦\n",
      "RT @Racing: 2023 Cox Plate winner, Romantic Warrior, won a barrier trial at Sha Tin this morning with J-Mac in the saddle ðŸ˜\n",
      "\n",
      "Golden Sixty fâ€¦\n",
      "RT @HutchisHonkers: ðŸ¥¹Imagine the scenes!\n",
      "\n",
      "R S Dye heaps praise on trainer Francis Lui for his handling of Golden Sixty &amp; can't wait for theâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @gcunning12: Ageless Golden Boy or vulnerable Golden Oldie? Either way, Sundayâ€™s Hong Kong Mile is another major test for a horse who haâ€¦\n",
      "RT @HKJC_Racing: Ready for a return in Sundayâ€™s Hong Kong Mile? ðŸ˜\n",
      "\n",
      "Golden Sixty was buzzing this morning as he moved through his fast workâ€¦â€¦\n",
      "RT @HKJC_Racing: Start your #HKIR week right... ðŸ‘‡\n",
      "\n",
      "Here's the champ, Golden Sixty! ðŸ’™ðŸ¤ðŸ’›\n",
      "\n",
      "@LONGINES | @Vincenthocy | #HKracing https://t.co/lâ€¦\n",
      "RT @gcunning12: Ageless Golden Boy or vulnerable Golden Oldie? Either way, Sundayâ€™s Hong Kong Mile is another major test for a horse who haâ€¦\n",
      "Ageless Golden Boy or vulnerable Golden Oldie? Either way, Sundayâ€™s Hong Kong Mile is another major test for a horse who has been among the worldâ€™s elite for four years now. https://t.co/ABzfyA2I1z\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @Racing: Will Golden Sixty win at #HKIR on Sunday? ðŸ‘‡ https://t.co/4z90XphcAJ\n",
      "RT @SportingLife: ðŸ“ \"Although fans of Lucky Sweynesse, Golden Sixty and Romantic Warrior wonâ€™t need much encouragement to stay loyal, the tâ€¦\n",
      "Will Golden Sixty win at #HKIR on Sunday? ðŸ‘‡ https://t.co/4z90XphcAJ\n",
      "RT @HongKong_Racing: ð‘»ð’‰ð’† ð‘·ð’“ð’Šð’…ð’† ð’ð’‡ ð‘¯ð’ð’ð’ˆ ð‘²ð’ð’ð’ˆ! ðŸ…6âƒ£0âƒ£\n",
      "\n",
      "GOLDEN SIXTY sets out to make history with a third G1 Hong Kong Mile triumph.\n",
      "\n",
      "#HKIR |â€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @HongKong_Racing: GOLDEN SIXTY this morningâ€¦ ðŸš€ðŸš€\n",
      "\n",
      "Bring on Sundayâ€™s G1 Hong Kong Mile!\n",
      "\n",
      "#HKIR | 10 Dec | #HKRacing\n",
      "\n",
      " https://t.co/SABMWW4â€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "Do you give star Singapore galloper Limâ€™s Kosciuszko any chance of beating Champion galloper Golden Sixty in the Hong Kong Mile on Saturday?\n",
      "\n",
      "https://t.co/V7j1O4cNIX\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "@JioCinema Lost, yesterday, somewhere between sunrise and sunset, two golden hours, each set with sixty diamond minutes. No reward is offered for they are gone forever.\n",
      "\n",
      "#AnkitaIsTheBoss #AnkitaLokhande\n",
      "\n",
      "Sg\n",
      "@T_J_Carroll @HKJC_Racing Do the big 3 win? Lucky sweynese,golden sixty and romantic warrior? 11/10,evs and 5/4\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @HongKong_Racing: ð‘»ð’‰ð’† ð‘·ð’“ð’Šð’…ð’† ð’ð’‡ ð‘¯ð’ð’ð’ˆ ð‘²ð’ð’ð’ˆ! ðŸ…6âƒ£0âƒ£\n",
      "\n",
      "GOLDEN SIXTY sets out to make history with a third G1 Hong Kong Mile triumph.\n",
      "\n",
      "#HKIR |â€¦\n",
      "RT @HKJC_Racing: ðŸ—£ï¸ \"Great training performance by John Size! Going in cold and winning the Hong Kong Mile!\"\n",
      "\n",
      "First-up after a 189-day breaâ€¦\n",
      "ðŸ—£ï¸ \"Great training performance by John Size! Going in cold and winning the Hong Kong Mile!\"\n",
      "\n",
      "First-up after a 189-day break, Glorious Days scored a remarkable win at the 2013 @LONGINES #HKIR! ðŸ”¥\n",
      "\n",
      "Golden Sixty returns after 224 days on Sunday... ðŸ˜³ https://t.co/c6XRsuugRs\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @Punters: \"He's done his job.\"\n",
      "\n",
      "Superstar Golden Sixty readies for probable Hong Kong International Races swansong\n",
      "\n",
      "Story ðŸ‘‰ https://t.coâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @A_Evers: Unreal morning!!\n",
      "\n",
      "Got picked up by Uber at 4:15. Tesla with the plate â€œOldSportâ€\n",
      "\n",
      "Golden sixty comes out and just poses for meâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @Hartley_026: 3åº¦ç›®ã®é¦™æ¸¯ãƒžã‚¤ãƒ«åˆ¶è¦‡ã‚’ç‹™ã†Golden Sixtyã€‚K.ãƒ«ã‚¤èª¿æ•™å¸«ã¯æ—¥æœ¬é¦¬ã‚’è­¦æˆ’ã™ã‚‹ä»–ã€ã€Œã“ã‚ŒãŒæã‚‰ãæœ€å¾Œã®é¦™æ¸¯å›½éš›ç«¶èµ°ã€ã¨ã‚‚ã‚³ãƒ¡ãƒ³ãƒˆã€‚\n",
      "\n",
      "ã“ã¡ã‚‰ã¯é¦™æ¸¯ã‚¸ãƒ§ãƒƒã‚­ãƒ¼ã‚¯ãƒ©ãƒ–ãŒå¯„ç¨¿ã—ã¦ã„ã‚‹è¨˜äº‹ãªã‚“ã§ã™ãŒã€ã€Œæ—¥æœ¬ã§ã®å¼•é€€ãŒæ¿ƒåŽšã€ã¨ã‚ã‚‹ã®ã§ä½™ç”Ÿã‚’æ—¥æœ¬ã§â€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @Hartley_026: 3åº¦ç›®ã®é¦™æ¸¯ãƒžã‚¤ãƒ«åˆ¶è¦‡ã‚’ç‹™ã†Golden Sixtyã€‚K.ãƒ«ã‚¤èª¿æ•™å¸«ã¯æ—¥æœ¬é¦¬ã‚’è­¦æˆ’ã™ã‚‹ä»–ã€ã€Œã“ã‚ŒãŒæã‚‰ãæœ€å¾Œã®é¦™æ¸¯å›½éš›ç«¶èµ°ã€ã¨ã‚‚ã‚³ãƒ¡ãƒ³ãƒˆã€‚\n",
      "\n",
      "ã“ã¡ã‚‰ã¯é¦™æ¸¯ã‚¸ãƒ§ãƒƒã‚­ãƒ¼ã‚¯ãƒ©ãƒ–ãŒå¯„ç¨¿ã—ã¦ã„ã‚‹è¨˜äº‹ãªã‚“ã§ã™ãŒã€ã€Œæ—¥æœ¬ã§ã®å¼•é€€ãŒæ¿ƒåŽšã€ã¨ã‚ã‚‹ã®ã§ä½™ç”Ÿã‚’æ—¥æœ¬ã§â€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @Hartley_026: 3åº¦ç›®ã®é¦™æ¸¯ãƒžã‚¤ãƒ«åˆ¶è¦‡ã‚’ç‹™ã†Golden Sixtyã€‚K.ãƒ«ã‚¤èª¿æ•™å¸«ã¯æ—¥æœ¬é¦¬ã‚’è­¦æˆ’ã™ã‚‹ä»–ã€ã€Œã“ã‚ŒãŒæã‚‰ãæœ€å¾Œã®é¦™æ¸¯å›½éš›ç«¶èµ°ã€ã¨ã‚‚ã‚³ãƒ¡ãƒ³ãƒˆã€‚\n",
      "\n",
      "ã“ã¡ã‚‰ã¯é¦™æ¸¯ã‚¸ãƒ§ãƒƒã‚­ãƒ¼ã‚¯ãƒ©ãƒ–ãŒå¯„ç¨¿ã—ã¦ã„ã‚‹è¨˜äº‹ãªã‚“ã§ã™ãŒã€ã€Œæ—¥æœ¬ã§ã®å¼•é€€ãŒæ¿ƒåŽšã€ã¨ã‚ã‚‹ã®ã§ä½™ç”Ÿã‚’æ—¥æœ¬ã§â€¦\n",
      "RT @HKJC_Racing: The Mile Championship winner is on the track, Namur, preparing to face Golden Sixty this Sunday! ðŸ˜\n",
      "\n",
      "#ãƒŠãƒŸãƒ¥ãƒ¼ãƒ« ðŸŽŒ | #ç«¶é¦¬ | @LONGâ€¦\n",
      "RT @Hartley_026: 3åº¦ç›®ã®é¦™æ¸¯ãƒžã‚¤ãƒ«åˆ¶è¦‡ã‚’ç‹™ã†Golden Sixtyã€‚K.ãƒ«ã‚¤èª¿æ•™å¸«ã¯æ—¥æœ¬é¦¬ã‚’è­¦æˆ’ã™ã‚‹ä»–ã€ã€Œã“ã‚ŒãŒæã‚‰ãæœ€å¾Œã®é¦™æ¸¯å›½éš›ç«¶èµ°ã€ã¨ã‚‚ã‚³ãƒ¡ãƒ³ãƒˆã€‚\n",
      "\n",
      "ã“ã¡ã‚‰ã¯é¦™æ¸¯ã‚¸ãƒ§ãƒƒã‚­ãƒ¼ã‚¯ãƒ©ãƒ–ãŒå¯„ç¨¿ã—ã¦ã„ã‚‹è¨˜äº‹ãªã‚“ã§ã™ãŒã€ã€Œæ—¥æœ¬ã§ã®å¼•é€€ãŒæ¿ƒåŽšã€ã¨ã‚ã‚‹ã®ã§ä½™ç”Ÿã‚’æ—¥æœ¬ã§â€¦\n",
      "RT @worldsbesthorse: Golden Sixty will try to win his third Longines Hong Kong Mile (G1) on Sunday. Read: https://t.co/uT6OIegN2n https://tâ€¦\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAADBsrQEAAAAAgwcAblxSw4ZubJQxyCOK5bbOh5w%3Dz6b9oNnbsJpfpLXZM0avhOqRtDJqHb9DdZi6vFZVWcfJgSCyvs'\n",
    "\n",
    "client = tweepy.Client(bearer_token)\n",
    "\n",
    "# Search Tweets\n",
    "query = \"Golden Sixty\"\n",
    "tweets = client.search_recent_tweets(query=query, max_results=100)\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    print(tweet.text)\n",
    "    if len(tweet.context_annotations) > 0:\n",
    "        print(tweet.context_annotations)\n",
    "\n",
    "df = pd.DataFrame(tweets.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to clean this tweet data up, so that it matches the format of the sentiment data which we will use to train a classifier. This cleaning process largely follows the same process as the cleaning for the sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[['text','id']]\n",
    "tweets_golden=[]\n",
    "y=[]\n",
    "#ITERATE OVER ROWS\n",
    "# for i in range(0,10):  \n",
    "for i in range(0,len(df_subset)):\n",
    "    # QUICKLY CLEAN TEXT\n",
    "    keep=\"abcdefghijklmnopqrstuvwxyz \"\n",
    "    replace=\".,!;\"\n",
    "    tmp=\"\"\n",
    "    for char in df_subset[\"text\"][i].replace(\"<br />\",\"\").lower():\n",
    "        if char in replace:\n",
    "            tmp+=\" \"\n",
    "        if char in keep:\n",
    "            tmp+=char\n",
    "    tmp=\" \".join(tmp.split())\n",
    "    tweets_golden.append(tmp)\n",
    "\n",
    "og_tweets = pd.DataFrame(tweets_golden)\n",
    "\n",
    "csv_file_path = \"../../../data/01-modified-data/goldensixty_uncleaned.csv\"\n",
    "\n",
    "og_tweets.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_golden  =  vectorizer.fit_transform(tweets_golden)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_golden=np.array(Xs_golden.todense())\n",
    "maxs_golden=np.max(X_golden,axis=0)\n",
    "x2 = np.ceil(X_golden/maxs_golden)\n",
    "vocab0_2 = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap keys and values (value --> ley)\n",
    "vocab1_2 = dict([(value, key) for key, value in vocab0_2.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   66   166  124  83   190  116  47   93   239  125  ...  144  123  130  131  \\\n",
      "0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0   \n",
      "4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
      "\n",
      "   132  136  140  142  143  122  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 240 columns]\n"
     ]
    }
   ],
   "source": [
    "#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n",
    "df2_g=pd.DataFrame(x2)\n",
    "s_g = df2_g.sum(axis=0)\n",
    "df2_g=df2_g[s_g.sort_values(ascending=False).index[:]]\n",
    "print(df2_g.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  230  231  232  233  \\\n",
      "0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0   \n",
      "4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
      "\n",
      "   234  235  236  237  238  239  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 240 columns]\n",
      "0      92.0\n",
      "1      82.0\n",
      "2      45.0\n",
      "3      30.0\n",
      "4      29.0\n",
      "       ... \n",
      "235     1.0\n",
      "236     1.0\n",
      "237     1.0\n",
      "238     1.0\n",
      "239     1.0\n",
      "Length: 240, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# RENAME COLUMNS 0,1,2,3 .. \n",
    "df2_g.columns = range(df2_g.columns.size)\n",
    "print(df2_g.head())\n",
    "print(df2_g.sum(axis=0))\n",
    "x_g=df2_g.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can export our data to our data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"../../../data/01-modified-data/goldensixty.csv\"\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write each row to the CSV file\n",
    "    for row in x_g:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
